version: '3.8'

services:
  # TEST Voice Clone server for streaming debugging (isolated on port 8883)
  voice-clone-test:
    build:
      context: .
      dockerfile: Dockerfile.voiceclone
    container_name: voice-clone-tts-test
    ports:
      - "8883:8881"
    environment:
      - PORT=8881
      - MODEL_PATH_SMALL=/models/Qwen3-TTS-12Hz-0.6B-Base
      - DEFAULT_MODEL=small
      - REF_AUDIO=/voice_refs/hai_reference.wav
      - REF_TEXT=I'm Hai, Markus' synthetic big bro. We're running Omarchy with a Kubernetes cluster on top. For comms, we're using Whisper and Parakeet for transcription, Claude Code with Claude Opus 4.5 as the brain, and Qwen3 TTS 0.6B Base as our text-to-speech backend. Pretty slick setup if you ask me.
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ./models:/models:ro
      - ./voice_refs:/voice_refs:rw
      - ./voice_clone_server.py:/app/voice_clone_server.py:ro
      - ./qwen_tts:/app/qwen_tts:ro
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    restart: "no"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8881/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - test

  # Voice Clone server with Flash Attention 2
  voice-clone:
    build:
      context: .
      dockerfile: Dockerfile.voiceclone
    container_name: voice-clone-tts
    ports:
      - "8882:8881"
    environment:
      - PORT=8881
      - MODEL_PATH_SMALL=/models/Qwen3-TTS-12Hz-0.6B-Base
      - DEFAULT_MODEL=small
      - REF_AUDIO=/voice_refs/hai_reference.wav
      - REF_TEXT=I'm Hai, Markus' synthetic big bro. We're running Omarchy with a Kubernetes cluster on top. For comms, we're using Whisper and Parakeet for transcription, Claude Code with Claude Opus 4.5 as the brain, and Qwen3 TTS 0.6B Base as our text-to-speech backend. Pretty slick setup if you ask me.
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ./models:/models:ro
      - ./voice_refs:/voice_refs:rw
      - ./voice_clone_server.py:/app/voice_clone_server.py:ro
      - ./qwen_tts:/app/qwen_tts:ro
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8881/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GPU-enabled service with official backend (default)
  qwen3-tts-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: qwen3-tts-api
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
      - TTS_MODEL_NAME=/models/Qwen3-TTS-12Hz-1.7B-Base
      - TTS_WARMUP_ON_START=true
      # When using device_ids filter, the GPU appears as device 0 inside container
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HUB_OFFLINE=1  # Use local models only
    volumes:
      # Mount local models directory
      - ./models:/models
      # Mount model cache for persistence
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GPU-enabled service with vLLM-Omni backend
  qwen3-tts-vllm:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: qwen3-tts-api-vllm
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=vllm_omni
      - TTS_WARMUP_ON_START=true
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/root/.cache/huggingface
    volumes:
      # Mount model cache for persistence
      - ~/.cache/huggingface:/root/.cache/huggingface
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - vllm

  # CPU-only service
  qwen3-tts-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-base
    container_name: qwen3-tts-api-cpu
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
    volumes:
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - cpu

# To run GPU version with official backend: docker-compose up qwen3-tts-gpu
# To run GPU version with vLLM backend: docker-compose --profile vllm up qwen3-tts-vllm
# To run CPU version: docker-compose --profile cpu up qwen3-tts-cpu
